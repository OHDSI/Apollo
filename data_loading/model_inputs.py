from abc import ABC, abstractmethod
from typing import Dict, Optional, Union, List

import numpy as np
from numpy.typing import NDArray

from data_loading.tokenizer import ConceptTokenizer
from data_loading.variable_names import DataNames, ModelInputNames
from model.model_settings import ModelSettings


def prefix_and_pad(sequence: NDArray[any],
                   prefix_value: any,
                   padding_value: any,
                   max_sequence_length: int
                   ) -> NDArray[any]:
    """
    Add a prefix and pad a sequence to a given length.

    Args
        sequence: The sequence to pad.
        prefix_value: The value to prefix with.
        max_sequence_length: The length to pad to (after prefixing).
        adding_value: The value to pad with.
    Returns
        The padded sequence.
    """
    n_to_pad = max_sequence_length - len(sequence) - 1  # Subtract one for the prefix
    if n_to_pad > 0:
        sequence = np.concatenate(([prefix_value], sequence, [padding_value] * n_to_pad), dtype=sequence.dtype)
    else:
        sequence = np.concatenate(([prefix_value], sequence), dtype=sequence.dtype)
    return sequence


def find_last_index(lst: List[any], value: any):
    for i in range(len(lst) - 1, -1, -1):
        if lst[i] == value:
            return i
    return -1  # Return -1 if the value is not found


class ModelInput(ABC):
    """
    Classes that can be used by the data transformer to generate input and expected output data for the model.
    """

    @abstractmethod
    def process_row(self,
                    row: Dict,
                    start_index: int,
                    end_index: int,
                    max_sequence_length: int) -> Dict[str, Union[np.ndarray, float]]:
        """
        Process a row to generate input or output data. The start and end index indicate a sequence with maximum length
         max_sequence_length - 1 to allow prefixing with a classification token.

        Args
            row: The row to process, as generated by the CDM processing.
            start_index: Any sequence in the row should start at this index.
            end_index: Any sequence in the row should end at this index.
            max_sequence_length: The maximum length of any sequence.

        Returns
            A dictionary from names to numpy arrays to be used by pytorch.
        """
        pass


class InputTransformer(ModelInput):

    def __init__(
            self,
            concept_tokenizer: Optional[ConceptTokenizer],
            visit_tokenizer: Optional[ConceptTokenizer],
            model_settings: ModelSettings
    ):
        """
        Initialization
        Args:
            concept_tokenizer: The tokenizer to use to tokenize the concepts. Should already be trained.
            visit_tokenizer: The tokenizer to use to tokenize the visits. Should already be trained.
            model_settings: The model settings.
        """
        self._tokenizer = concept_tokenizer
        self._visit_tokenizer = visit_tokenizer
        self._model_settings = model_settings

    def process_row(self,
                    row: Dict,
                    start_index: int,
                    end_index: int,
                    max_sequence_length: int) -> Dict[str, Union[np.ndarray, float]]:
        model_inputs = {}

        if self._model_settings.concept_embedding:
            concept_ids = np.array(row[DataNames.CONCEPT_IDS][start_index:end_index], dtype=str)
            token_ids = self._tokenizer.encode(concept_ids)
            token_ids = prefix_and_pad(sequence=token_ids,
                                       prefix_value=self._tokenizer.get_classification_token_id(),
                                       padding_value=self._tokenizer.get_padding_token_id(),
                                       max_sequence_length=max_sequence_length)
            model_inputs[ModelInputNames.TOKEN_IDS] = token_ids

        if self._model_settings.visit_concept_embedding:
            visit_ids = np.array(row[DataNames.VISIT_CONCEPT_IDS][start_index:end_index], dtype=str)
            visit_token_ids = self._visit_tokenizer.encode(visit_ids)
            visit_token_ids = prefix_and_pad(sequence=visit_token_ids,
                                             prefix_value=self._visit_tokenizer.get_classification_token_id(),
                                             padding_value=self._visit_tokenizer.get_padding_token_id(),
                                             max_sequence_length=max_sequence_length)
            model_inputs[ModelInputNames.VISIT_TOKEN_IDS] = visit_token_ids

        if self._model_settings.segment_embedding:
            visit_segments = np.array(row[DataNames.VISIT_SEGMENTS][start_index:end_index])
            visit_segments = prefix_and_pad(sequence=visit_segments,
                                            prefix_value=0,
                                            padding_value=0,
                                            max_sequence_length=max_sequence_length)
            model_inputs[ModelInputNames.VISIT_SEGMENTS] = visit_segments

        if self._model_settings.visit_order_embedding:
            visit_orders = np.array(row[DataNames.VISIT_CONCEPT_ORDERS][start_index:end_index])
            # Normalize the visit_orders using the smallest visit_concept_orders. Add 1 for CLS token:
            visit_orders = visit_orders - min(visit_orders) + 1
            visit_orders = prefix_and_pad(sequence=visit_orders,
                                          prefix_value=0,
                                          padding_value=max_sequence_length - 1,
                                          max_sequence_length=max_sequence_length)
            model_inputs[ModelInputNames.VISIT_CONCEPT_ORDERS] = visit_orders

        if self._model_settings.age_embedding:
            ages = np.array(row[DataNames.AGES][start_index:end_index], dtype=np.float32)
            ages = prefix_and_pad(sequence=ages,
                                  prefix_value=0,
                                  padding_value=0,
                                  max_sequence_length=max_sequence_length)
            model_inputs[ModelInputNames.AGES] = ages

        if self._model_settings.date_embedding:
            dates = np.array(row[DataNames.DATES][start_index:end_index], dtype=np.float32)
            dates = prefix_and_pad(sequence=dates,
                                   prefix_value=0,
                                   padding_value=0,
                                   max_sequence_length=max_sequence_length)
            model_inputs[ModelInputNames.DATES] = dates

        padding_mask = prefix_and_pad(sequence=np.zeros(shape=end_index - start_index, dtype=np.float32),
                                      prefix_value=0,
                                      padding_value=-np.inf,
                                      max_sequence_length=max_sequence_length)
        model_inputs[ModelInputNames.PADDING_MASK] = padding_mask

        return model_inputs
