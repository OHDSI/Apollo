[system]

# Path where data was stored using the GeneralPretrainedModelTools package:
sequence_data_folder = d:/GPM_Sim/pretraining/patient_sequence

# Path where the model files will be written:
output_folder = d:/GPM_Sim/pretraining/model

# Path to a pre-trained model files to load. If empty, a new model will be created:
pretrained_model_folder =

[data preparation]

batch_size = 32

max_sequence_length = 512

# If the input sequence is longer than max_sequence_length, truncate the sequence "random" or keep the "tail":
truncate_type = random

[learning objectives]

# Predict a random subset of the input concepts that are masked out:
masked_concept_learning = yes

# For masked concept learning, only mask at most one concept per visit:
mask_one_concept_per_visit = yes

# Predict a random subset of the input visit concepts that are masked out:
masked_visit_concept_learning = yes

# Predict a given label for each person:
label_prediction = no

[training]

# Split the data into training and validation sets, and compute the validation performance after each epoch:
do_evaluation = yes

# Fraction of the data to use for training:
train_fraction = 0.8

# Number of epochs to train for:
num_epochs = 100

# Number of epochs to freeze the pretrained model for:
num_freeze_epochs = 1

learning_rate = 0.001

weight_decay = 0.01

[model]

# Size of hidden layers: (default: 768)
hidden_size = 768

# Number of hidden layers: (default: 12)
num_hidden_layers = 12

# Number of attention heads: (default: 12)
num_attention_heads = 12

# Size of intermediate layers: (default: 3072)
intermediate_size = 3072

# Type of activation function used in the intermediate layer: (default: gelu)
hidden_act = gelu

# Dropout probability for the hidden layers: (default: 0.1)
hidden_dropout_prob = 0.1

# Dropout probability for the attention layer: (default: 0.1)
attention_probs_dropout_prob = 0.1
